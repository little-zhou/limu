{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "811d542d-a997-4bf6-9e82-c238d302cd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1be93a7-0434-4e0c-bc23-ca5faea08ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 缩放点积注意力\n",
    "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
    "    # 每个单词所映射到的维度\n",
    "    embed_size = Q.size(-1)\n",
    "    # (batch_size, seq_len, embed_size)\n",
    "    # 计算点积并且缩放\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(embed_size)\n",
    "    # 有掩码的话就对掩码进行填充\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "    # 对缩放后的分数应用Softmax函数,得到注意力权重\n",
    "    atten_weights = F.softmax(scores, dim=-1)\n",
    "    # 1.加权求和计算输出 2.返回注意力权重\n",
    "    return atten_weights @ V, atten_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c22fcd47-dc3f-48e0-8ac4-641b8506609f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 单头注意力机制\n",
    "# class Attention(nn.Module):\n",
    "#     def __init__(self, x):\n",
    "#         super(Attention, self).__init__()\n",
    "#         self.x = x\n",
    "\n",
    "#     def forward(mask=None):\n",
    "#         embed_size = x.size(-1)\n",
    "        \n",
    "#         Q = nn.Linear(embed_size, embed_size)\n",
    "#         K = nn.Linear(embed_size, embed_size)\n",
    "#         V = nn.Linear(embed_size, embed_size)\n",
    "\n",
    "#         output, atten_weights = scaled_dot_product_attention(Q, K, V, mask)\n",
    "        \n",
    "#         return output, atten_weights\n",
    "\n",
    "# 单头注意力机制\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "        # 定义线性层,用于生成查询、键和值矩阵\n",
    "        self.w_q = nn.Linear(embed_size, embed_size)\n",
    "        self.w_k = nn.Linear(embed_size, embed_size)\n",
    "        self.w_v = nn.Linear(embed_size, embed_size)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        # 将输入序列通过线性变换,变成 Q K V 矩阵\n",
    "        Q = self.w_q(q)\n",
    "        K = self.w_k(k)\n",
    "        V = self.w_v(v)\n",
    "\n",
    "        # 缩放点积注意力 得到值已经注意力权重\n",
    "        output, atten_weights = scaled_dot_product_attention(Q, K, V, mask)\n",
    "\n",
    "        return output, atten_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66f2c136-04bb-47fd-8807-d2f0a488c04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自注意力机制\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.attention = Attention(embed_size)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # 在自注意力机制中,q、k、v都来自于同一个输入序列\n",
    "        output, atten_weights = self.attention(x, x, x, mask)\n",
    "        return output, atten_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b13488a-0143-4bd7-8a7a-a019b8a58570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 交叉注意力\n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        self.embed_size = embed_size\n",
    "        self.attention = Attention(embed_size)\n",
    "\n",
    "    def forward(self, q, kv, mask=None):\n",
    "        # 在交叉注意力中,q来自于解码器,kv来自于编码器\n",
    "        output, atten_weights = self.attention(q, kv, kv, mask)\n",
    "        return output, atten_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df8c6e96-66d3-4392-bf09-a5c5c67652a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多头注意力机制\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        assert self.embed_size % self.num_heads == 0\n",
    "\n",
    "        self.d_k = self.embed_size // self.num_heads\n",
    "\n",
    "        # 定义线性层,用于生成q k v 矩阵\n",
    "        self.w_q = nn.Linear(embed_size, embed_size)\n",
    "        self.w_k = nn.Linear(embed_size, embed_size)\n",
    "        self.w_v = nn.Linear(embed_size, embed_size)\n",
    "\n",
    "        # 输出线性层,用于将多头拼接后的输出映射回embed_size\n",
    "        self.fc_out = nn.Linear(embed_size, embed_size)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        n_batch = q.size(0)\n",
    "        # (batch_size, seq_len, embed_size) -> (batch_size, num_heads, seq_len, head_nums)\n",
    "        query = self.w_q(q).view(n_batch, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        key = self.w_k(k).view(n_batch, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        value = self.w_v(v).view(n_batch, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # x的形状 (batch_size, num_heads, seq_len_q, d_v)\n",
    "        x, atten_weights = scaled_dot_product_attention(query, key, value, mask)\n",
    "        # (batch_size, seq_len, embed_size)\n",
    "        x = x.transpose(1, 2).contiguous().view(n_batch, -1, self.num_heads * self.d_k)\n",
    "        # (batch_size, seq_len, embed_size)\n",
    "        return self.fc_out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "42a2d3bc-02a8-4857-ba5f-9946681f3ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "num_heads = 2\n",
    "seq_len_q = 3 # 查询序列长度\n",
    "seq_len_k = 3 # 键序列长度\n",
    "head_dim = 4\n",
    "\n",
    "# 模拟查询矩阵 Q 和键值矩阵 K, V\n",
    "Q = torch.randn(batch_size, num_heads, seq_len_q, head_dim)\n",
    "K = torch.randn(batch_size, num_heads, seq_len_k, head_dim)\n",
    "V = torch.randn(batch_size, num_heads, seq_len_k, head_dim)\n",
    "\n",
    "# 生成下三角掩码矩阵(1, 1, seq_len_q, seq_len_k),通过广播应用到所有头\n",
    "mask = torch.tril(torch.ones(seq_len_q, seq_len_k)).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "# 执行缩放点积注意力,并应用下三角掩码\n",
    "output, atten_weights = scaled_dot_product_attention(Q, K, V, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cb69a050-7e8a-41a4-ac7c-2e58788674c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1.0000, 0.0000, 0.0000],\n",
       "          [0.5771, 0.4229, 0.0000],\n",
       "          [0.1815, 0.2287, 0.5898]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000],\n",
       "          [0.4707, 0.5293, 0.0000],\n",
       "          [0.8778, 0.0980, 0.0242]]],\n",
       "\n",
       "\n",
       "        [[[1.0000, 0.0000, 0.0000],\n",
       "          [0.9347, 0.0653, 0.0000],\n",
       "          [0.1374, 0.4930, 0.3695]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000],\n",
       "          [0.3168, 0.6832, 0.0000],\n",
       "          [0.3102, 0.4320, 0.2579]]]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atten_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cd02aaaa-6dda-4e35-bc4c-573cbb1dc18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FFN 前馈神经网络\n",
    "# self.w_2(self.dropout(self.w_1(x).relu()))\n",
    "# 本质就是两层线性网络,加一个relu函数\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    位置前馈网络\n",
    "\n",
    "    参数:\n",
    "        d_model: 输入和输出向量的维度\n",
    "        d_ff: FNN隐藏层的维度,或者说中间层\n",
    "        dropout: 随机失活率(Dropout),即随机屏蔽部分神经元的输出,用于防止过拟合\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        # self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.relu(self.w_1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "99887c8d-de8e-4a83-9ad1-52ccb08c7ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 残差连接,避免梯度消失\n",
    "class ResidualConnection(nn.Module):\n",
    "    def __init__(self, dropout=0.1):\n",
    "        super(ResidualConnection, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        return x + self.dropout(sublayer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1cb0910c-e8b9-48c6-94d9-59f4abd1b6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    # Output = γx + β 其中γ和β是可学习的参数,用于进一步调整归一化后的输出\n",
    "    def __init__(self, feature_size, epsilon=1e-9):\n",
    "        # 可学习缩放参数,初始值为1\n",
    "        self.gamma = nn.Parameter(torch.ones(feature_size))\n",
    "        # 可学习偏移参数,初始值为0\n",
    "        self.beta = nn.Parameter(torch.zeros(feature_size))\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        std = x.std(dim=-1, keepdim=True)\n",
    "        return self.gamma * (x - mean) / (std + self.epsilon) + self.beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "24421a7f-8a4d-42dc-9c51-a02ab67c6346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 残差网络和归一化\n",
    "# Output = LayerNorm(x + Sublayer(x))\n",
    "class SublayerConnection(nn.Module):\n",
    "    def __init__(self, feature_size, dropout=0.1, epsilon=1e-9):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.redidual = ResidualConnection(dropout) # 使用ResidualConnection 进行残差连接\n",
    "        self.norm = LayerNorm(feature_size, epsilon) # 层归一化\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        # 将子层输出应用dropout后经过残差连接后再进行归一化\n",
    "        return self.norm(self.redidual(x, sublayer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7ec83fd3-e821-4c03-90a3-6c0dc2a0fb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    def __init__(self, feature_size, dropout=0.1, epsilon=1e-9):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(feature_size, epsilon)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        # 将子层输出到应用dropout后经过残差连接后再进行归一化\n",
    "        return self.norm(x + self.dropout(sublayer(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "22fd73ae-a914-49f5-823c-a8f73c2a8280",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.scale_factor = math.sqrt(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embed(x) * self.scale_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e2f3e639-3a98-4d05-8afc-e68d520537c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1) # 位置索引 (max_len, 1)\n",
    "\n",
    "        # 计算每个维度对应的频率\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0)/d_model))\n",
    "        \n",
    "        # 位置和频率相结合\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        # 增加一个维度方便后续相加, 形状变为 (1, max_len, d_model)\n",
    "        pe = pe.unsqueeze(0)\n",
    "\n",
    "        #将位置编码注册为模型的缓冲区,不作为参数更新\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 取出与输入序列长度相同的部分位置编码，并与输入相加\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b80c4858-8762-4a54-8bbe-e42014e17e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SourceEmbedding(nn.Module):\n",
    "    def __init__(self, src_vocab_size, d_model, dropout=0.1):\n",
    "        super(SourceEmbedding, self).__init__()\n",
    "        self.embed = Embeddings(src_vocab_size, d_model) # 词嵌入层\n",
    "        self.positional_encoding = PositionalEncoding(d_model, dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        return self.positional_encoding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3757231f-1ffe-4d1d-b78b-9650008de8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TargetEmbedding(nn.Module):\n",
    "    def __init__(self, tgt_vocab_size, d_model, dropout=0.1):\n",
    "        super(TargetEmbedding, self).__init__()\n",
    "        self.embed = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        return self.positional_encoding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "23d27b16-4b7b-46cb-b393-6bb491d11187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding Mask 填充掩码\n",
    "def create_padding_mask(seq, pad_token_id=0):\n",
    "    # seq的形状为(batch_size, seq_len)\n",
    "    mask = (seq != pad_token_id).unsqueeze(1).unsqueeze(2) # (batch_size, 1, 1, seq_len)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "95565af4-0415-4d67-a41b-fefde6d2f9a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ True,  True,  True, False, False]]],\n",
      "\n",
      "\n",
      "        [[[ True,  True, False, False, False]]]])\n"
     ]
    }
   ],
   "source": [
    "# seq = torch.tensor([[5, 7, 9, 0, 0], [8, 6, 0, 0, 0]])  # 0 表示 <PAD>\n",
    "# print(create_padding_mask(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "710c7aa5-298c-4446-b919-95ed69c0ec53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 未来信息掩码 Look-ahead Mask\n",
    "def create_look_ahead_mask(size):\n",
    "    # 下三角矩阵\n",
    "    mask = torch.tril(torch.ones(size, size)).type(torch.bool) \n",
    "    return mask # (seq_len, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0b81aa80-316e-4cd9-a6a3-e32ebc33aed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ True, False, False, False, False],\n",
      "        [ True,  True, False, False, False],\n",
      "        [ True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True]])\n"
     ]
    }
   ],
   "source": [
    "print(create_look_ahead_mask(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9e92c0c7-8f4a-4212-80fa-2089293b7fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 组合掩码,我们需要将填充掩码和未来信息掩码进行组合,同时实现两种掩码的效果\n",
    "def create_decoder_mask(tgt_seq, pad_token_id=0):\n",
    "    # (batch_size, 1, 1, seq_len_tgt)\n",
    "    padding_mask = create_padding_mask(tgt_seq, pad_token_id)\n",
    "    # (seq_len_tgt, seq_len_tgt)\n",
    "    look_ahead_mask = create_look_ahead_mask(tgt_seq.size(1)).to(tgt_seq.device)\n",
    "\n",
    "    # 广播机制 pytorch会从右往左对齐\n",
    "    # (batch_size, 1, seq_len_tgt, seq_len_tgt)\n",
    "    combined_mask = look_ahead_mask.unsqueeze(0) & padding_mask\n",
    "    return combined_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d4883235-0f95-4ad4-a7d5-537108932f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tgt_seq = torch.tensor([[1, 2, 3, 4, 0]])  # 0 表示 <PAD>\n",
    "# print(create_decoder_mask(tgt_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "942f9442-1544-4421-992a-39886a6db3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, h, d_ff, dropout):\n",
    "        \"\"\"\n",
    "        编码层\n",
    "\n",
    "        参数:\n",
    "            d_model: 嵌入维度\n",
    "            h: 多头注意力的头数\n",
    "            d_ff: 前馈神经网络的隐藏层维度\n",
    "            dropout: Dropout 概率\n",
    "        \"\"\"\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_atten = MultiHeadAttention(d_model, h)\n",
    "        self.feed_forward = PositionwiseFeedForward(d_model, d_ff)\n",
    "\n",
    "        # 定义两个子层,分别用于多头注意力跟前馈神经网络\n",
    "        self.sublayers = nn.ModuleList([SublayerConnection(d_model, dropout) for _ in range(2)])\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x, src_mask):\n",
    "        # 自注意力层\n",
    "        x = self.sublayers[0](x, lambda x: self.self_atten(x, x, x, src_mask))\n",
    "        # 前馈子层\n",
    "        x = self.sublayers[1](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2f014cf0-b915-4da7-a005-c20545ffde3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, h, d_ff, dropout):\n",
    "        \"\"\"\n",
    "        解码器\n",
    "\n",
    "        参数:\n",
    "            d_model: 嵌入维度\n",
    "            h: 多头注意力的头数\n",
    "            d_ff: 前馈神经网络的隐藏层维度\n",
    "            dropout: Dropout 概率\n",
    "        \"\"\"\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        # 带掩码的多头注意力\n",
    "        self.self_attn = MultiHeadAttention(d_model, h)\n",
    "        # 交叉注意力\n",
    "        # self.cross_attn = CrossAttention(d_model)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, h)\n",
    "        # 前馈神经网络\n",
    "        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "\n",
    "        # 定义三个子层连接,分别用于掩码多头注意力、多头注意力和前馈神经网络\n",
    "        self.sublayers = nn.ModuleList([SublayerConnection(d_model, dropout) for _ in range(3)])\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        # 第一个子层: 掩码多头自注意力\n",
    "        x = self.sublayers[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
    "        \n",
    "        # 第二层: 交叉多头注意力\n",
    "        x = self.sublayers[1](x, lambda x: self.cross_attn(x, memory, memory, src_mask))\n",
    "\n",
    "        # 第三层: 前馈神经网络\n",
    "        x = self.sublayers[2](x, self.feed_forward)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "97d062a5-4d86-4873-8b71-6137ac40fc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, d_model, N, h, d_ff, dropout=0.1):\n",
    "        \"\"\"\n",
    "        编码器,由N个EncoderLayer堆叠而成\n",
    "\n",
    "        参数:\n",
    "            d_model: 嵌入维度\n",
    "            N: 编码器层的数量\n",
    "            h: 多头注意力的头数\n",
    "            d_ff: 前馈神经网络的隐藏层维度\n",
    "            dropout: Dropout 概率\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = nn.ModuleList([EncoderLayer(d_model, h, d_ff, dropout) for _ in range(N)])\n",
    "        self.norm = LayerNorm(d_model) # 最后一层再做一次归一化\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x) # 最后层归一化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69fa0a4-133a-4bf0-a880-c38ee060e209",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, d_model, N, h, d_ff, dropout=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model, h, dropout) for _ in range(N)])\n",
    "        self.norm = LayerNorm(d_model) # 最后一层归一化\n",
    "\n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        \"\"\"\n",
    "        前向传播函数。\n",
    "        \n",
    "        参数:\n",
    "            x: 解码器输入 (batch_size, seq_len_tgt, d_model)\n",
    "            memory: 编码器的输出 (batch_size, seq_len_src, d_model)\n",
    "            src_mask: 用于交叉注意力的源序列掩码\n",
    "            tgt_mask: 用于自注意力的目标序列掩码\n",
    "            \n",
    "        返回:\n",
    "            解码器的输出\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, src_mask, tgt_mask)\n",
    "        return self.norm(x) # 最后一层归一化"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
